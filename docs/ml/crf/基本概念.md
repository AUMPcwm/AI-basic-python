## 条件随机场CRF(一)从随机场到线性链条件随机场
### 简介
条件随机场（conditional random field，简称 CRF），是一种鉴别式机率模型，是随机场的一种，常用于标注或分析序列资料，如自然语言文字或是生物序列。 如同马尔可夫随机场，条件随机场为无向性之图模型，图中的顶点代表随机变量，顶点间的连线代表随机变量间的相依关系，在条件随机场当中，随机变量 Y 的分布为条件机率，给定的观察值则为随机变量 X。原则上，条件随机场的图模型布局是可以任意给定的，一般常用的布局是链接式的架构，链接式架构不论在训练（training）、推论（inference）、或是解码（decoding）上，都存在有效率的算法可供演算。

条件随机场跟隐马尔可夫模型常被一起提及，条件随机场对于输入和输出的机率分布，没有如隐马尔可夫模型那般强烈的假设存在。 线性链条件随机场应用于标注问题是由Lafferty等人与2001年提出的。

### 数学描述
对于CRF，我们给出准确的数学语言描述：设X与Y是随机变量，P(Y|X)是给定X时Y的条件概率分布，若随机变量Y构成的是一个马尔科夫随机场，则称条件概率分布P(Y|X)是条件随机场。
> 首先，我们来看看什么是随机场。“随机场”的名字取的很玄乎，其实理解起来不难。随机场是由若干个位置组成的整体，当给每一个位置中按照某种分布随机赋予一个值之后，其全体就叫做随机场。还是举词性标注的例子：假如我们有一个十个词形成的句子需要做词性标注。这十个词每个词的词性可以在我们已知的词性集合（名词，动词...)中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。

>了解了随机场，我们再来看看马尔科夫随机场。马尔科夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅仅与和它相邻的位置的赋值有关，和与其不相邻的位置的赋值无关。继续举十个词的句子词性标注的例子：　如果我们假设所有词的词性只和它相邻的词的词性有关时，这个随机场就特化成一个马尔科夫随机场。比如第三个词的词性除了与自己本身的位置有关外，只与第二个词和第四个词的词性有关。

>理解了马尔科夫随机场，再理解CRF就容易了。CRF是马尔科夫随机场的特例，它假设马尔科夫随机场中只有X和Y两种变量，X一般是给定的，而Y一般是在给定X的条件下我们的输出。这样马尔科夫随机场就特化成了条件随机场。在我们十个词的句子词性标注的例子中，X是词，Y是词性。因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个CRF。

注意在CRF的定义中，我们并没有要求X和Y有相同的结构。而实现中，我们一般都假设X和Y有相同的结构，即: X=(X1,X2,...Xn),Y=(Y1,Y2,...Yn)


在我们的十个词的句子的词性标记中，词有十个，词性也是十个，因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个linear-CRF。

我们再来看看linear-CRF的数学定义：设$X=(X_1,X_2,...X_n),Y=(Y_1,Y_2,...Y_n)$均为线性链表示的随机变量序列，在给定随机变量序列X的情况下，随机变量Y的条件概率分布$P(Y|X)$构成条件随机场，即满足马尔科夫性：$P(Y_i|X,Y_1,Y_2, Y_n)=P(Y_i|X,Y_{i−1},Y_{i+1})$, 则称$P(Y|X)$为线性链条件随机场。

### 参数化
对于linear-CRF，我们如何将其转化为可以学习的机器学习模型呢？这是通过特征函数和其权重系数来定义的。什么是特征函数呢？

在linear-CRF中，特征函数分为两类:
>+ 第一类是定义在Y节点上的节点特征函数，这类特征函数只和当前节点有关，记为：$s_l(y_i,x,i),l=1,2,...L$, 其中L是定义在该节点的节点特征函数的总个数，i是当前节点在序列的位置。
>+ 第二类是定义在Y上下文的局部特征函数，这类特征函数只和当前节点和上一个节点有关，记为：$t_k(y_{i−1},y_i,x,i),k=1,2,...K$, 其中K是定义在该节点的局部特征函数的总个数，i是当前节点在序列的位置。之所以只有上下文相关的局部特征函数，没有不相邻节点之间的特征函数，是因为我们的linear-CRF满足马尔科夫性。

**无论是节点特征函数还是局部特征函数，它们的取值只能是0或者1。即满足特征条件或者不满足特征条件。** 同时，我们可以为每个特征函数赋予一个权值，用以表达我们对这个特征函数的信任度. 假设$t_k$的权重系数是$λ_k, s_l$的权重系数是$μ_l$,则linear-CRF由我们所有的$t_k,λ_k,s_l,μ_l$共同决定。

此时我们得到了linear-CRF的参数化形式如下：

$$P(y|x)=1/Z(x) \times exp(∑_{i,k}λ_k t_k(y_{i−1},y_i,x,i)+∑_{i,l}μ_l s_l(y_i,x,i))$$

其中，Z(x)为规范化因子：$Z(x)=∑_y exp(∑_{i,k} λ_k t_k(y_{i−1},y_i,x,i)+∑_{i,l} μ_l s_l(y_i,x,i))$.

回到特征函数本身，**每个特征函数定义了一个linear-CRF的规则，则其系数定义了这个规则的可信度。所有的规则和其可信度一起构成了我们的linear-CRF的最终的条件概率分布。**

这里我们给出一个linear-CRF用于词性标注的实例，为了方便，我们简化了词性的种类。假设输入的都是三个词的句子，即$X=(X_1,X_2,X_3)$, 输出的词性标记为$Y=(Y_1,Y_2,Y_3)$,其中$Y∈{1(名词)，2(动词)}$.

这里只标记出取值为1的特征函数如下：

$$ t_1=t_1(y_{i−1}=1,y_i=2,x,i),i=2,3,λ_1=1 \ t_2=t_2(y_1=1,y_2=1,x,2),λ_2=0.5 \ t_3=t_3(y_2=2,y_3=1,x,3),λ_3=1 \ t_4=t_4(y_1=2,y_2=1,x,2),λ_4=1 \ t_5=t_5(y_2=2,y_3=2,x,3),λ_5=0.2 \ s_1=s_1(y_1=1,x,1),μ_1=1 \ s_2=s_2(y_i=2,x,i),i=1,2,μ_2=0.5 \ s_3=s_3(y_i=1,x,i),i=2,3,μ_3=0.8 \ s_4=s_4(y_3=2,x,3)μ_4=0.5 $$

> 对于第一个特征函数: $t_1=t_1(y_{i−1}=1,y_i=2,x,i),i=2,3,λ_1=1$, 在$y_{i−1}=1,y_i=2,i=2,3$这个条件下$t_1=1$。对于$t_1$函数，在$i=1$的时候，由于不成立，那么此时$t_1=0$,这样的特征函数我们没有标出。
> **特征函数部分要按需求而定。实际应用中是已知的数据序列的一些特征信息，通过提取抽象而来的。** 所以虽然是自己定义，但是不是凭空定义，而是从数据的实际情况提取的，能提取的越多肯定越好。特征函数写成这个样子是一个通用的写法.

求标记(1,2,2)的非规范化概率。

利用linear-CRF的参数化公式，我们有：

$$ P(y|x)∝exp[∑_{k=1}^5 λ_k ∑_{i=2}^3 t_k(y_{i−1},y_i,x,i)+∑_{l=1}^4 μ_l∑_{i=1}^3 s_l(y_i,x,i)] $$

带入(1,2,2)我们有：$P(y_1=1,y_2=2,y_3=2|x)∝exp(3.2)$

在前面，我们用$s_l$表示节点特征函数，用$t_k$表示局部特征函数，同时也用了不同的符号表示权重系数，导致表示起来比较麻烦。其实我们可以对特征函数稍加整理，将其统一起来。

假设我们在某一节点我们有$K_1$个局部特征函数和$K_2$个节点特征函数，总共有$K=K_1+K_2$个特征函数。我们用一个特征函数$f_k(y_{i−1},y_i,x,i)$来统一表示如下:

$$ f_k(y_{i−1},y_i,x,i) = \left\{\begin{matrix} t_k(y_{i−1},y_i,x,i) & k=1,2,...K_1 \ s_l(y_i,x,i) & k=K_{1+l} & l=1,2...,K2 \end{matrix}\right. $$


对$f_k(y_{i−1},y_i,x,i)$在各个序列位置求和得到：$f_k(y,x)=∑_{i=1}^n f_k(y_{i−1},y_i,x,i)$, 同时我们也统一$f_k(y_{i−1},y_i,x,i)$对应的权重系数$w_k$如下：

$$ w_k=\left\{\begin{matrix} λ_k & k=1,2,...K_1 \ μ_l & k=K_1+l & l=1,2...,K_2 \end{matrix}\right. $$

这样，我们的linear-CRF的参数化形式简化为：$P(y|x)=1/Z(x) \times exp(∑_{k=1}^K w_k f_k(y,x))$, 其中，$Z(x)$为规范化因子：$Z(x)=∑_y exp(∑_{k=1}^K w_k f_k(y,x))$.

### 向量化
如果将上两式中的$w_k$与$f_k$的用向量表示，即: $w=(w_1,w_2,...w_K)^T, F(y,x)=(f_1(y,x),f_2(y,x),...f_K(y,x))^T$, 则linear-CRF的参数化形式简化为内积形式如下：

$$ P_w(y|x)=exp(w∙F(y,x))/Z_w(x)=exp(w∙F(y,x))/∑_yexp(w∙F(y,x)) $$

### 矩阵化
将上一节统一后的linear-CRF公式加以整理，我们还可以将linear-CRF的参数化形式写成矩阵形式。为此我们对于观测序列x的每一个位置$i=1,2,...,n+1$定义一个m×m的矩阵M，m为y所有可能的状态的取值个数。M定义如下：

$$ M_i(x)=[M_i(y_{i−1},y_i|x)]=[exp(W_i(y_{i−1},y_i|x))]=[exp(∑_{k=1}^K w_k f_k(y_{i−1},y_i,x,i))] $$

> 即对于y中每个位置$y_i$,可以得到一个非规范化概率的矩阵，矩阵中的每一个位置(a,b)对应从$y_{i-1}$的对应取值(a)到$y_i$的对应取值(b)的非规范化转移概率。

我们引入起点和终点标记y$0=start,y_{n+1}=stop$, 这样，标记序列y的非规范化概率可以通过n+1个矩阵元素的乘积得到，即：

$$ P_w(y|x)=1/Z_w(x) \times ∏_{i=1}^{n+1}M_i(y_{i−1},y_i|x) $$

>关于这里的乘积: 加上一个定好的$(y_0,y_{n+1})$,就是 $y_0*M_1(y_1,y_0|x)M_2(y_2,y_1|x)...*M_{n+1}(y_{n+1},y_n|x)$, 也就是观测到$(y_0,y_1,...y_n,y_{n+1})$的联合分布概率，只是是没有规范化的而已。

因为标签之间的关系就是当前与之前的转移关系, 所以说, 总体的联合分布, 可以认为是所有的独立的转移对概率的乘积举个简单的例子: $P(x_1, x_2, x_3, ..., x_n)=P(x_1)P(x_2|x_1)...P(x_n|x_1, x_2, ..., x_{n-1})=P(x_1|x_0)P(x_2|x_1)...P(x_n|x_{n-1})= \prod^n P(x_i|x_{i-1})$

其中$Z_w(x)$为规范化因子。

以上就是linear-CRF的模型基础

### 基本问题
linear-CRF有三个类似的的基本问题。在linear-CRF中，我们对于给出的观测序列x是一直作为一个整体看待的，也就是不会拆开看(x1,x2,...)，因此linear-CRF的问题模型要比HMM简单一些，那么CRF的这三个问题的求解就不难了。
>+ linear-CRF第一个问题是评估，即给定linear-CRF的条件概率分布$P(y|x)$, 在给定输入序列x和输出序列y时，计算条件概率$P(y_i|x)$和$P(y_{i−1}，y_i|x)$以及对应的期望.
>+ linear-CRF第二个问题是学习，即给定训练数据集X和Y，学习linear-CRF的模型参数$w_k$和条件概率$P_w(y|x)$，这个问题的求解比HMM的学习算法简单的多，普通的梯度下降法，拟牛顿法都可以解决。
>+ linear-CRF第三个问题是解码，即给定linear-CRF的条件概率分布$P(y|x)$,和输入序列x, 计算使条件概率最大的输出序列y。使用维特比算法可以很方便的解决这个问题.

在问题1和问题3，特征函数的权重是已知的。在问题2，特征函数的权重是需要求的模型参数。
### 应用
>条件随机场为具有无向的图模型，图中的顶点代表随机变量，顶点间的连线代表随机变量间的相依关系，在条件随机场中，随机变量 Y 的分布为条件机率，给定的观察值则为随机变量 X。将条件随机场应用于图像分割，则我们将一张图片用无向图表示，每个像素点为无向图中的顶点， 像素间的连接关系为顶点的连线。图像分割的过程，就是将每个顶点赋予不同的label（目标或背景），即，将无向图中的边在边界处正确地切分开。在图片分割当中，具有相似位置和颜色特征的两个像素，其被赋上相同label的概率大，则被分割的可能性小，这就对应了条件随机场中的概率模型。除此以外，条件随机场可以对所有特征进行全局归一化，能够求得全局的最优解。这就对应到图割当中的，能量函数求最小值。所以，应用条件随机场，求图像分割后的能量最小值，即可得到全局的图像分割结果。