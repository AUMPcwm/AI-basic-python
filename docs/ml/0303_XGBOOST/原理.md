# XGBOOST原理与实战

标签（空格分隔）： 机器学习

---

##XGBOOST算法原理：
全称：eXtreme Gradient Boosting.
xgboost算法的步骤和GB基本相同，都是首先初始化为一个常数，gb是根据一阶导数ri，xgboost是根据一阶导数gi和二阶导数hi，迭代生成基学习器，相加更新学习器。
###Gradient Boosting
 GB是一种Boosting的方法，其与传统的Boosting的区别是，每一次的计算是为了减少上一次残差（residual），为了消除残差，可以在残差的梯度（Gradient）方向上建立一个新的模型。所以说，在Gradient Boosting中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boosting对正确、错误样本进行加权有着很大的区别。
 
 ####通用的Gradient Descent Boosting的框架：
  $$F(x;P)=\sum_{m=1}^n \beta_mh(x;\alpha_m)              $$

对于模型的参数 $\{\beta, \alpha\}$,我们可以用下面的式子来表示，这个式子的意思是，对于N各样本点$\{x_i, y_i\}$ 计算其在模型$ F(x;\alpha,\beta)$ 下的损失函数，最优的$\{\alpha, \beta  \}$ 就是能够使得这个损失函数最小的$\{\alpha, \beta  \}$。
表示两个m维的参数：
$$  \beta_m=arg\ min \sum_{i=1}^N L(y_i,F_{m-1}(x_i)+\beta h(x_i;\alpha_m))              $$

写成梯度下降的方式就是下面的形式，也就是我们将要得到的模型fm(x)的参数$\{ am,bm\}$能够使得fm的方向是之前得到的模型$F_{m-1}(x)$的算式函数下降最快的方向：
 对于每一个数据点$x_i$都可以得到一个$g_m(x_i)$，最终我们得到一个完整梯度下降方向
 $$    g_m=\{   -g_m(x_i)\}_{1}^N          $$
 $$  -g_m(x_i)=-[\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}]_{f(x)=F_{m-1}(x)}        $$

为了使得fm(x) 能够在gm(x)的方向上，我们可以优化下面的式子得到，可以使得最小二乘法：
$$  \alpha_m=arg\ min \sum_{i=1}^N(-g_m(x_i)-\beta h(x;\alpha))^2                    $$

得到了$\alpha$ 的基础上，可以得到$\beta m$
$$ \beta_m=arg\ min \sum_{i=1}^N L(y_i,F_{m-1} (x_i)+\beta h(x_i;\alpha_m))         $$

最终合并到模型中：
 $$  F_m(x)=F_{m-1} (x)+\rho_m h(x;\alpha_m)$$
 
 算法的流程图如下：
 ---
 ```
 Algorithom 1: Gradient_Boost
 $$  F_0$$
 ```
 ---
 
 
##XGBOOST 的优化：
####1. xgboost实现了一种近似的算法。
####2. 为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率。
####3. 特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。
####4. 将数据收集到线程内部的buffer，然后再计算，提高算法的效率。
####5. 结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。

## XGBOOST 的优势
####1、正则化
####2、并行处理
####3、高度灵活性
####4、缺失值处理
####5、剪枝
>* 当分裂时遇到一个负损失时，GBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。 
####6、内置交叉验证


## XCBOOST 实战

//xgboost加载数据为DMatrix对象
dtrain = xgb.DMatrix(x_train, y_train)
dtest = xgb.DMatrix(x_test)

//xgboost交叉验证并输出rmse
cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=100, early_stopping_rounds=20,
verbose_eval=50, show_stdv=False)
cv_output[[‘train-rmse-mean’, ‘test-rmse-mean’]].plot() 
//xgboost训练模型
num_boost_rounds = len(cv_output)
model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)

//xgboost参数设置

>* params 这是一个字典，里面包含着训练中的参数关键字和对应的值，形式是params ={‘booster’:’gbtree’,’eta’:0.1}
>* dtrain 训练的数据
>* num_boost_round 这是指提升迭代的个数
>* evals 这是一个列表形式是evals = [(dtrain,’train’),(dval,’val’)]或者是evals =
    [(dtrain,’train’)],对于第一种情况，它使得我们可以在训练过程中观察验证集的效果
>* obj,自定义目的函数
>* feval,自定义评估函数
>* maximize,是否对评估函数进行最大化
>* early_stopping_rounds,早期停止次数假设为100，验证集的误差迭代到一定程度在100次内不能再继续降低，就停止迭代
>* verbose_eval,如果为True,则evals中元素的评估结果会输出在结果中；如果输入数字，假设为5，则每隔5个迭代输出一次。
>* learning_rates 每一次提升的学习率
>* xgb_model ,在训练之前用于加载的xgb model

//显示xgboost模型中比较重要的几个feature
featureImportance = model.get_fscore()
features = pd.DataFrame()
features[‘features’] = featureImportance.keys()
features[‘importance’] = featureImportance.values()
features.sort_values(by=[‘importance’],ascending=False,inplace=True)
fig,ax= plt.subplots()
fig.set_size_inches(20,10)
plt.xticks(rotation=60)
sn.barplot(data=features.head(30),x=”features”,y=”importance”,ax=ax,orient=”v”) 


















2017.08-至今         前隆科技有限公司                                            数学建模师  
>*	丰富内部数据采集，深化外部数据合作。利用多维内外部数据建立模型宽表，建立特征工程；
>*	通过催收笔记，优化风险分类，提纯欺诈目标；
>*	使用LR，RF，GBDT，XGBOOST，半监督等算法，构建信用评分模型，包括A评分卡，反欺诈模型和长生命周期模型；
>*	搭建模型监控Dashboard，监控已上线模型KS,ROC,PSI,LIFT等指标；
>*	利用内外部建模分数，构建强大的集成ensemble模型。
